{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab64a20-1f8d-4256-a790-98bfb0bf0ac6",
   "metadata": {},
   "source": [
    "# Projet Prévision Météo en Australie - MLOps Juillet 2024\n",
    "\n",
    "Ce projet déploie un modèle **Random Forest** dans une application de prévision de pluie à J+1 sur une ville donnée en Australie. Le projet intègre des outils MLOps tels que **Airflow**, **MLflow** pour la pipeline data-model, **Prometheus** et **Grafana** pour le monitoring des ressources machines, ainsi que **FastAPI** et **Streamlit** pour l'inférence.\n",
    "\n",
    "## Table des matières\n",
    "\n",
    "- [Description du projet](#description-du-projet)\n",
    "- [Architecture du projet](#architecture-du-projet)\n",
    "- [Les DAGs Airflow](#les-dags-airflow)\n",
    "- [Outils utilisés](#outils-utilisés)\n",
    "- [Installation et utilisation](#installation-et-utilisation)\n",
    "  - [Version de production](#version-de-production)\n",
    "  - [Version de développement](#version-de-développement)\n",
    "- [CI/CD](#cicd)\n",
    "- [Monitoring](#monitoring)\n",
    "- [Contribution](#contribution)\n",
    "- [Licence](#licence)\n",
    "- [Contact](#contact)\n",
    "\n",
    "---\n",
    "\n",
    "## Description du projet\n",
    "\n",
    "Le projet vise à prédire la probabilité de pluie le lendemain pour une ville spécifique en Australie. Il s'appuie sur un modèle **Random Forest** entraîné sur des données météorologiques actualisées quotidiennement. Les principaux composants du projet sont :\n",
    "\n",
    "- **Airflow** pour orchestrer les pipelines de données (ETL) et l'entraînement du modèle.\n",
    "- **MLflow** pour gérer les expériences de machine learning et suivre les performances des modèles.\n",
    "- **FastAPI** et **Streamlit** pour fournir une interface utilisateur pour les prédictions et une interface administrateur pour gérer les mises à jour et les entraînements.\n",
    "- **Prometheus** et **Grafana** pour le monitoring des ressources serveurs et la visualisation des métriques.\n",
    "- Utilisation de **Docker** pour la containerisation et de **Docker Hub** pour le déploiement des images.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture du projet\n",
    "\n",
    "![Architecture du Projet][] <!-- Assurez-vous d'inclure une image représentant l'architecture de votre projet -->\n",
    "\n",
    "Le projet est entièrement containerisé, ce qui facilite le déploiement et la scalabilité. L'architecture se compose des éléments suivants :\n",
    "\n",
    "- **Scraping des données** : Récupération quotidienne des relevés météorologiques via des scripts Python.\n",
    "- **Pipeline ETL avec Airflow** : Extraction, transformation et chargement des données dans une base de données PostgreSQL.\n",
    "- **Entraînement du modèle avec MLflow** : Entraînement hebdomadaire du modèle Random Forest, comparaison avec le modèle précédent selon le F1-score, et déploiement du meilleur modèle.\n",
    "- **API d'inférence avec FastAPI** : Fournit des prédictions basées sur le modèle déployé.\n",
    "- **Interface utilisateur avec Streamlit** : Permet aux utilisateurs de faire des prédictions et aux administrateurs de lancer manuellement une recuperation des données du jour et un entrainement selection du meilleur modele.\n",
    "- **Monitoring avec Prometheus et Grafana** : Collecte et visualisation des métriques du système et des performances du modèle.\n",
    "- **CI/CD avec GitHub Actions** : Tests automatisés et déploiement continu sur Docker Hub.\n",
    "\n",
    "---\n",
    "\n",
    "## Les DAGs Airflow\n",
    "\n",
    "- **DAG de collecte des données (quotidien)**\n",
    "  - **Tâches** :\n",
    "    - Scraping du site météorologique pour obtenir les relevés journaliers.\n",
    "    - Nettoyage et préparation des données.\n",
    "    - Insertion des données dans la base de données PostgreSQL.\n",
    "- **DAG d'entraînement du modèle (hebdomadaire)**\n",
    "  - **Tâches** :\n",
    "    - Chargement des données depuis la base de données.\n",
    "    - Entraînement du modèle Random Forest avec MLflow.\n",
    "    - Comparaison avec le modèle précédent en utilisant le F1-score.\n",
    "    - Enregistrement du meilleur modèle pour l'inférence.\n",
    "- **DAG combiné (exécution manuelle)**\n",
    "  - **Tâches** :\n",
    "    - Exécution des tâches de collecte des données.\n",
    "    - Entraînement du modèle et sélection du meilleur.\n",
    "  - **Utilisation** :\n",
    "    - Peut être déclenché depuis le panneau administrateur de l'application Streamlit pour forcer une mise à jour du modèle.\n",
    "- **DAG de tests unitaires**\n",
    "  - **Tâches** :\n",
    "    - Exécution de la suite de tests pour valider le bon fonctionnement des pipelines et du modèle.\n",
    "\n",
    "---\n",
    "\n",
    "## Outils utilisés\n",
    "\n",
    "- **Langage** : Python 3.8+\n",
    "- **Outils MLOps** :\n",
    "  - **Apache Airflow** : Orchestration des pipelines ETL et des entraînements.\n",
    "  - **MLflow** : Gestion des expériences de machine learning et suivi des modèles.\n",
    "- **Développement Web** :\n",
    "  - **FastAPI** : Création de l'API d'inférence.\n",
    "  - **Streamlit** : Interface utilisateur pour les prédictions et les actions administratives.\n",
    "- **Monitoring** :\n",
    "  - **Prometheus** : Collecte des métriques système.\n",
    "  - **Grafana** : Visualisation des métriques via des tableaux de bord.\n",
    "- **Gestion des données** :\n",
    "  - **PostgreSQL** : Base de données pour stocker les données préparées.\n",
    "- **Containerisation et Déploiement** :\n",
    "  - **Docker** et **Docker Compose** : Containerisation des services.\n",
    "  - **Docker Hub** : Stockage et distribution des images Docker.\n",
    "  - **GitHub Actions** : Intégration continue et déploiement continu (CI/CD).\n",
    "\n",
    "---\n",
    "\n",
    "## Installation et utilisation\n",
    "\n",
    "### Pré-requis\n",
    "\n",
    "- **Docker** et **Docker Compose** installés sur votre machine.\n",
    "- **Make** installé pour utiliser les Makefiles.\n",
    "\n",
    "### Version de production\n",
    "\n",
    "1. **Initialiser Airflow** :\n",
    "\n",
    "   ```bash\n",
    "   make -f Makefile.prod init-airflow\n",
    "   ```\n",
    "\n",
    "2. **Démarrer les services** :\n",
    "\n",
    "   ```bash\n",
    "   make -f Makefile.prod start\n",
    "   ```\n",
    "\n",
    "### Version de développement\n",
    "\n",
    "1. **Initialiser Airflow** :\n",
    "\n",
    "   ```bash\n",
    "   make -f Makefile.dev init-airflow\n",
    "   ```\n",
    "\n",
    "2. **Démarrer les services** :\n",
    "\n",
    "   ```bash\n",
    "   make -f Makefile.dev start\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## CI/CD\n",
    "\n",
    "Le projet utilise **GitHub Actions** pour l'intégration continue et le déploiement continu :\n",
    "\n",
    "- **Tests automatisés** : À chaque push ou pull request, les tests unitaires sont exécutés pour s'assurer que le code est fonctionnel.\n",
    "- **Build des images Docker** : Les images Docker sont construites et testées.\n",
    "- **Déploiement sur Docker Hub** : Si les tests réussissent, les images sont poussées sur Docker Hub avec un nouveau tag de version.\n",
    "\n",
    "---\n",
    "\n",
    "## Monitoring\n",
    "\n",
    "**Prometheus** collecte les métriques système, telles que l'utilisation du CPU, de la mémoire et des ressources réseau. **Grafana** est utilisé pour visualiser ces métriques à travers des tableaux de bord personnalisables.\n",
    "\n",
    "- **Accéder à Grafana** :\n",
    "\n",
    "  Rendez-vous sur `http://localhost:3000` et connectez-vous avec les identifiants par défaut (configurés dans le docker-compose).\n",
    "\n",
    "- **Dashboard permettant de visualiser entre autres** :\n",
    "\n",
    "  - Utilisation du CPU.\n",
    "  - Utilisation de la mémoire.\n",
    "  - Utilisation disque.\n",
    "  - Utilisation réseau.\n",
    "  - Performances des services Docker\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Licence\n",
    "\n",
    "Ce projet est sous licence MIT - voir le fichier [LICENSE](./LICENSE) pour plus de détails.\n",
    "\n",
    "---\n",
    "\n",
    "Ce projet a été développé par l'équipe suivante :\n",
    "\n",
    "- Shirley GERVOLINO ([GitHub](https://github.com/Shirley687) / [LinkedIn](https://www.linkedin.com/in/))\n",
    "- Tristan ([GitHub](https://github.com/tristandatascience) / [LinkedIn](https://www.linkedin.com/in/))\n",
    "- Prudence Amani ([GitHub](https://github.com/) / [LinkedIn](https://www.linkedin.com/in/))\n",
    "- Stéphane Los ([GitHub](https://github.com/hil-slos) / [LinkedIn](https://fr.linkedin.com/in/losstephane/))\n",
    "\n",
    "---\n",
    "\n",
    "*Ce projet a été réalisé dans le cadre du programme MLOps de Juillet 2024.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
