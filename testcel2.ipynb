{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab64a20-1f8d-4256-a790-98bfb0bf0ac6",
   "metadata": {},
   "source": [
    "# Projet Prévision Météo en Australie - MLOps Juillet 2024\n",
    "\n",
    "Ce projet déploie un modèle **Random Forest** dans une application de prévision de pluie à J+1 sur une ville donnée en Australie. Le projet intègre des outils MLOps tels que **Airflow**, **MLflow** pour la pipeline data-model, **Prometheus** et **Grafana** pour le monitoring des ressources machines, ainsi que **FastAPI** et **Streamlit** pour l'inférence.\n",
    "\n",
    "## Table des matières\n",
    "\n",
    "- [Description du projet](#description-du-projet)\n",
    "- [Architecture du projet](#architecture-du-projet)\n",
    "- [Les DAGs Airflow](#les-dags-airflow)\n",
    "- [Outils utilisés](#outils-utilisés)\n",
    "- [Installation et utilisation](#installation-et-utilisation)\n",
    "  - [Version de production](#version-de-production)\n",
    "  - [Version de développement](#version-de-développement)\n",
    "- [CI/CD](#cicd)\n",
    "- [Monitoring](#monitoring)\n",
    "- [Contribution](#contribution)\n",
    "- [Licence](#licence)\n",
    "- [Contact](#contact)\n",
    "\n",
    "---\n",
    "\n",
    "## Description du projet\n",
    "\n",
    "Le projet vise à prédire la probabilité de pluie le lendemain pour une ville spécifique en Australie. Il s'appuie sur un modèle **Random Forest** entraîné sur des données météorologiques actualisées quotidiennement. Les principaux composants du projet sont :\n",
    "\n",
    "- **Airflow** pour orchestrer les pipelines de données (ETL) et l'entraînement du modèle.\n",
    "- **MLflow** pour gérer les expériences de machine learning et suivre les performances des modèles.\n",
    "- **FastAPI** et **Streamlit** pour fournir une interface utilisateur pour les prédictions et une interface administrateur pour gérer les mises à jour et les entraînements.\n",
    "- **Prometheus** et **Grafana** pour le monitoring des ressources serveurs et la visualisation des métriques.\n",
    "- Utilisation de **Docker** pour la containerisation et de **Docker Hub** pour le déploiement des images.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture du projet\n",
    "\n",
    "![Architecture du Projet][] <!-- Assurez-vous d'inclure une image représentant l'architecture de votre projet -->\n",
    "\n",
    "Le projet est entièrement containerisé, ce qui facilite le déploiement et la scalabilité. L'architecture se compose des éléments suivants :\n",
    "\n",
    "- **Scraping des données** : Récupération quotidienne des relevés météorologiques via des scripts Python.\n",
    "- **Pipeline ETL avec Airflow** : Extraction, transformation et chargement des données dans une base de données PostgreSQL.\n",
    "- **Entraînement du modèle avec MLflow** : Entraînement hebdomadaire du modèle Random Forest, comparaison avec le modèle précédent selon le F1-score, et déploiement du meilleur modèle.\n",
    "- **API d'inférence avec FastAPI** : Fournit des prédictions basées sur le modèle déployé.\n",
    "- **Interface utilisateur avec Streamlit** : Permet aux utilisateurs de faire des prédictions et aux administrateurs de lancer manuellement une recuperation des données du jour et un entrainement selection du meilleur modele.\n",
    "- **Monitoring avec Prometheus et Grafana** : Collecte et visualisation des métriques du système et des performances du modèle.\n",
    "- **CI/CD avec GitHub Actions** : Tests automatisés et déploiement continu sur Docker Hub.\n",
    "\n",
    "---\n",
    "\n",
    "## Les DAGs Airflow\n",
    "\n",
    "- **DAG de collecte des données (quotidien)**\n",
    "  - **Tâches** :\n",
    "    - Scraping du site météorologique pour obtenir les relevés journaliers.\n",
    "    - Nettoyage et préparation des données.\n",
    "    - Insertion des données dans la base de données PostgreSQL.\n",
    "- **DAG d'entraînement du modèle (hebdomadaire)**\n",
    "  - **Tâches** :\n",
    "    - Chargement des données depuis la base de données.\n",
    "    - Entraînement du modèle Random Forest avec MLflow.\n",
    "    - Comparaison avec le modèle précédent en utilisant le F1-score.\n",
    "    - Enregistrement du meilleur modèle pour l'inférence.\n",
    "- **DAG combiné (exécution manuelle)**\n",
    "  - **Tâches** :\n",
    "    - Exécution des tâches de collecte des données.\n",
    "    - Entraînement du modèle et sélection du meilleur.\n",
    "  - **Utilisation** :\n",
    "    - Peut être déclenché depuis le panneau administrateur de l'application Streamlit pour forcer une mise à jour du modèle.\n",
    "- **DAG de tests unitaires**\n",
    "  - **Tâches** :\n",
    "    - Exécution de la suite de tests pour valider le bon fonctionnement des pipelines et du modèle.\n",
    "\n",
    "---\n",
    "\n",
    "## Outils utilisés\n",
    "\n",
    "- **Langage** : Python 3.8+\n",
    "- **Outils MLOps** :\n",
    "  - **Apache Airflow** : Orchestration des pipelines ETL et des entraînements.\n",
    "  - **MLflow** : Gestion des expériences de machine learning et suivi des modèles.\n",
    "- **Développement Web** :\n",
    "  - **FastAPI** : Création de l'API d'inférence.\n",
    "  - **Streamlit** : Interface utilisateur pour les prédictions et les actions administratives.\n",
    "- **Monitoring** :\n",
    "  - **Prometheus** : Collecte des métriques système.\n",
    "  - **Grafana** : Visualisation des métriques via des tableaux de bord.\n",
    "- **Gestion des données** :\n",
    "  - **PostgreSQL** : Base de données pour stocker les données préparées.\n",
    "- **Containerisation et Déploiement** :\n",
    "  - **Docker** et **Docker Compose** : Containerisation des services.\n",
    "  - **Docker Hub** : Stockage et distribution des images Docker.\n",
    "  - **GitHub Actions** : Intégration continue et déploiement continu (CI/CD).\n",
    "\n",
    "---\n",
    "\n",
    "## Installation et utilisation\n",
    "\n",
    "### Pré-requis\n",
    "\n",
    "- **Docker** et **Docker Compose** installés sur votre machine.\n",
    "- **Make** installé pour utiliser les Makefiles.\n",
    "\n",
    "### Version de production\n",
    "\n",
    "1. **Initialiser Airflow** :\n",
    "\n",
    "   ```bash\n",
    "   make -f Makefile.prod init-airflow\n",
    "   ```\n",
    "\n",
    "2. **Démarrer les services** :\n",
    "\n",
    "   ```bash\n",
    "   make -f Makefile.prod start\n",
    "   ```\n",
    "\n",
    "### Version de développement\n",
    "\n",
    "1. **Initialiser Airflow** :\n",
    "\n",
    "   ```bash\n",
    "   make -f Makefile.dev init-airflow\n",
    "   ```\n",
    "\n",
    "2. **Démarrer les services** :\n",
    "\n",
    "   ```bash\n",
    "   make -f Makefile.dev start\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## CI/CD\n",
    "\n",
    "Le projet utilise **GitHub Actions** pour l'intégration continue et le déploiement continu :\n",
    "\n",
    "- **Tests automatisés** : À chaque push ou pull request, les tests unitaires sont exécutés pour s'assurer que le code est fonctionnel.\n",
    "- **Build des images Docker** : Les images Docker sont construites et testées.\n",
    "- **Déploiement sur Docker Hub** : Si les tests réussissent, les images sont poussées sur Docker Hub avec un nouveau tag de version.\n",
    "\n",
    "---\n",
    "\n",
    "## Monitoring\n",
    "\n",
    "**Prometheus** collecte les métriques système, telles que l'utilisation du CPU, de la mémoire et des ressources réseau. **Grafana** est utilisé pour visualiser ces métriques à travers des tableaux de bord personnalisables.\n",
    "\n",
    "- **Accéder à Grafana** :\n",
    "\n",
    "  Rendez-vous sur `http://localhost:3000` et connectez-vous avec les identifiants par défaut (configurés dans le docker-compose).\n",
    "\n",
    "- **Dashboard permettant de visualiser entre autres** :\n",
    "\n",
    "  - Utilisation du CPU.\n",
    "  - Utilisation de la mémoire.\n",
    "  - Utilisation disque.\n",
    "  - Utilisation réseau.\n",
    "  - Performances des services Docker\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Licence\n",
    "\n",
    "Ce projet est sous licence MIT - voir le fichier [LICENSE](./LICENSE) pour plus de détails.\n",
    "\n",
    "---\n",
    "\n",
    "Ce projet a été développé par l'équipe suivante :\n",
    "\n",
    "- Shirley GERVOLINO ([GitHub](https://github.com/Shirley687) / [LinkedIn](https://www.linkedin.com/in/))\n",
    "- Tristan ([GitHub](https://github.com/tristandatascience) / [LinkedIn](https://www.linkedin.com/in/))\n",
    "- Prudence Amani ([GitHub](https://github.com/) / [LinkedIn](https://www.linkedin.com/in/))\n",
    "- Stéphane Los ([GitHub](https://github.com/hil-slos) / [LinkedIn](https://fr.linkedin.com/in/losstephane/))\n",
    "\n",
    "---\n",
    "\n",
    "*Ce projet a été réalisé dans le cadre du programme MLOps de Juillet 2024.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389713a-4116-45b7-982b-ca2c9c88861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pipenv install dlt[lancedb]==0.5.1a0\n",
    "!pipenv install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbc120-f01c-4a59-ae2d-f85a0ca22392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import dlt\n",
    "\n",
    "qa_dataset = requests.get(\"https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1\").json()\n",
    "\n",
    "@dlt.resource\n",
    "def qa_documents():\n",
    "  for course in qa_dataset:\n",
    "    yield course[\"documents\"]\n",
    "\n",
    "pipeline = dlt.pipeline(pipeline_name=\"from_json\", destination=\"lancedb\", dataset_name=\"qanda\")\n",
    "\n",
    "load_info = pipeline.run(qa_documents, table_name=\"documents\")\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d7232-1085-4f72-9058-4b3afbe7ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"./.lancedb\")\n",
    "print(db.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c499ef-3657-45e7-a817-f308173932cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "!pipenv install dlt[lancedb]==0.5.1a0\n",
    "!pipenv install sentence-transformers\n",
    "import requests\n",
    "import dlt\n",
    "\n",
    "qa_dataset = requests.get(\"https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1\").json()\n",
    "\n",
    "@dlt.resource\n",
    "def qa_documents():\n",
    "  for course in qa_dataset:\n",
    "    yield course[\"documents\"]\n",
    "\n",
    "pipeline = dlt.pipeline(pipeline_name=\"from_json\", destination=\"lancedb\", dataset_name=\"qanda\")\n",
    "\n",
    "load_info = pipeline.run(qa_documents, table_name=\"documents\")\n",
    "\n",
    "print(load_info)\n",
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"./.lancedb\")\n",
    "print(db.table_names())\n",
    "db_table = db.open_table(\"qanda___documents\")\n",
    "\n",
    "db_table.to_pandas()\n",
    "import os\n",
    "from dlt.destinations.adapters import lancedb_adapter\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL_PROVIDER\"] = \"sentence-transformers\"\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "pipeline = dlt.pipeline(pipeline_name=\"from_json_embedded\", destination=\"lancedb\", dataset_name=\"qanda_embedded\")\n",
    "\n",
    "load_info = pipeline.run(lancedb_adapter(qa_documents, embed=[\"text\", \"question\"]), table_name=\"documents\")\n",
    "print(load_info)\n",
    "db = lancedb.connect(\"./.lancedb\")\n",
    "print(db.table_names())\n",
    "db_table = db.open_table(\"qanda_embedded___documents\")\n",
    "\n",
    "db_table.to_pandas()\n",
    "!yes | dlt init rest_api lancedb\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"SOURCES__REST_API__NOTION__API_KEY\"] = userdata.get(\"SOURCES__REST_API__NOTION__API_KEY\")\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL_PROVIDER\"] = \"sentence-transformers\"\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__URI\"] = \".lancedb\"\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "class PostBodyPaginator(BasePaginator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cursor = None\n",
    "\n",
    "    def update_state(self, response: Response) -> None:\n",
    "        # Assuming the API returns an empty list when no more data is available\n",
    "        if not response.json():\n",
    "            self._has_next_page = False\n",
    "        else:\n",
    "            self.cursor = response.json().get(\"next_cursor\")\n",
    "            if self.cursor is None:\n",
    "                self._has_next_page = False\n",
    "\n",
    "    def update_request(self, request: Request) -> None:\n",
    "        if request.json is None:\n",
    "            request.json = {}\n",
    "\n",
    "        # Add the cursor to the request body\n",
    "        request.json[\"start_cursor\"] = self.cursor\n",
    "\n",
    "@dlt.resource(name=\"employee_handbook\")\n",
    "def rest_api_notion_resource():\n",
    "    notion_config: RESTAPIConfig = {\n",
    "        \"client\": {\n",
    "            \"base_url\": \"https://api.notion.com/v1/\",\n",
    "            \"auth\": {\n",
    "                \"token\": dlt.secrets[\"sources.rest_api.notion.api_key\"]\n",
    "            },\n",
    "            \"headers\":{\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Notion-Version\": \"2022-06-28\"\n",
    "            }\n",
    "        },\n",
    "        \"resources\": [\n",
    "            {\n",
    "                \"name\": \"search\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"search\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"paginator\": PostBodyPaginator(),\n",
    "                    \"json\": {\n",
    "                        \"query\": \"Homework: Employee handbook\",\n",
    "                        \"filter\": {\n",
    "                            \"property\": \"object\",\n",
    "                            \"value\": \"page\"\n",
    "                        },\n",
    "                        \"sort\": {\n",
    "                            \"direction\": \"ascending\",\n",
    "                            \"timestamp\": \"last_edited_time\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"data_selector\": \"results\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_content\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"blocks/{page_id}/children\",\n",
    "                    \"paginator\": JSONResponsePaginator(),\n",
    "                    \"params\": {\n",
    "                        \"page_id\": {\n",
    "                            \"type\": \"resolve\",\n",
    "                            \"resource\": \"search\",\n",
    "                            \"field\": \"id\"\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    yield from rest_api_source(notion_config,name=\"employee_handbook\")\n",
    "\n",
    "def extract_page_content(response):\n",
    "    block_id = response[\"id\"]\n",
    "    last_edited_time = response[\"last_edited_time\"]\n",
    "    block_type = response.get(\"type\", \"Not paragraph\")\n",
    "    if block_type != \"paragraph\":\n",
    "        content = \"\"\n",
    "    else:\n",
    "        try:\n",
    "            content = response[\"paragraph\"][\"rich_text\"][0][\"plain_text\"]\n",
    "        except IndexError:\n",
    "            content = \"\"\n",
    "    return {\n",
    "        \"block_id\": block_id,\n",
    "        \"block_type\": block_type,\n",
    "        \"content\": content,\n",
    "        \"last_edited_time\": last_edited_time,\n",
    "        \"inserted_at_time\": datetime.now(timezone.utc)\n",
    "    }\n",
    "\n",
    "@dlt.resource(\n",
    "    name=\"employee_handbook\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"block_id\",\n",
    "    columns={\"last_edited_time\":{\"dedup_sort\":\"desc\"}}\n",
    "    )\n",
    "def rest_api_notion_incremental(\n",
    "    last_edited_time = dlt.sources.incremental(\"last_edited_time\", initial_value=\"2024-06-26T08:16:00.000Z\",primary_key=(\"block_id\"))\n",
    "):\n",
    "    # last_value = last_edited_time.last_value\n",
    "    # print(last_value)\n",
    "\n",
    "    for block in rest_api_notion_resource.add_map(extract_page_content):\n",
    "        if not(len(block[\"content\"])):\n",
    "            continue\n",
    "        yield block\n",
    "\n",
    "def load_notion() -> None:\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"company_policies\",\n",
    "        destination=\"lancedb\",\n",
    "        dataset_name=\"notion_pages\",\n",
    "        # full_refresh=True\n",
    "    )\n",
    "\n",
    "    load_info = pipeline.run(\n",
    "        lancedb_adapter(\n",
    "            rest_api_notion_incremental,\n",
    "            embed=\"content\"\n",
    "        ),\n",
    "        table_name=\"homework\",\n",
    "        write_disposition=\"merge\"\n",
    "    )\n",
    "    print(load_info)\n",
    "\n",
    "load_notion()\n",
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\".lancedb\")\n",
    "dbtable = db.open_table(\"notion_pages___homework\")\n",
    "\n",
    "dbtable.to_pandas()\n",
    "dbtable.to_pandas().sort_values(by=\"last_edited_time\", ascending=False)\n",
    "#Q1 14 \n",
    "#Q2 2024-07-05 23:33:00+00:00\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!nohup ollama serve > nohup.out 2>&1 &\n",
    "%%capture\n",
    "!ollama pull llama2-uncensored\n",
    "!pip install ollama\n",
    "import ollama\n",
    "\n",
    "def retrieve_context_from_lancedb(dbtable, question, top_k=2):\n",
    "\n",
    "    query_results = dbtable.search(query=question).to_list()\n",
    "    context = \"\\n\".join([result[\"content\"] for result in query_results[:top_k]])\n",
    "\n",
    "    return context\n",
    "\n",
    "def main():\n",
    "  # Connect to the lancedb table\n",
    "  db = lancedb.connect(\".lancedb\")\n",
    "  dbtable = db.open_table(\"notion_pages___employee_handbook\")\n",
    "\n",
    "  # A system prompt telling ollama to accept input in the form of \"Question: ... ; Context: ...\"\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant that helps users understand policies inside a company's employee handbook. The user will first ask you a question and then provide you relevant paragraphs from the handbook as context. Please answer the question based on the provided context. For any details missing in the paragraph, encourage the employee to contact the HR for that information. Please keep the responses conversational.\"}\n",
    "  ]\n",
    "\n",
    "  while True:\n",
    "    # Accept user question\n",
    "    question = input(\"You: how many PTO days are the employees entitled to in a year?\")\n",
    "\n",
    "    # Retrieve the relevant paragraphs on the question\n",
    "    context = retrieve_context_from_lancedb(dbtable,question,top_k=2)\n",
    "\n",
    "    # Create a user prompt using the question and retrieved context\n",
    "    messages.append(\n",
    "        {\"role\": \"user\", \"content\": f\"Question: '{question}'; Context:'{context}'\"}\n",
    "    )\n",
    "\n",
    "    # Get the response from the LLM\n",
    "    response = ollama.chat(\n",
    "        model=\"llama2-uncensored\",\n",
    "        messages=messages\n",
    "    )\n",
    "    response_content = response['message']['content']\n",
    "    print(f\"Assistant: {response_content}\")\n",
    "\n",
    "    # Add the response into the context window\n",
    "    messages.append(\n",
    "        {\"role\": \"assistant\", \"content\":response_content}\n",
    "    )\n",
    "main()\n",
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80924d97-570f-433d-ae5e-f0bc21bdce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('essai print \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1d0c4-dbc3-4bdc-afa9-f80c603b1fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
